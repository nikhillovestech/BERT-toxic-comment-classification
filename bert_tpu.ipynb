{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw Toxic Comment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model checkpoints and other required files from this link: https://drive.google.com/open?id=1iLyI3BgjO8H5tf96WI__C1MmOrujzAtE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams():\n",
    "  return tf.contrib.training.HParams(\n",
    "      bert_config_file = 'gs://neuron/data/bert_config.json',\n",
    "      model_dir = 'gs://neuron/bert',\n",
    "      init_checkpoint = 'gs://neuron/data/bert_model.ckpt',\n",
    "      train_file = 'gs://neuron/data/bert_1304874.tfrecords',\n",
    "      test_file = 'gs://neuron/data/bert_test.tfrecords',\n",
    "      pred_file = 'gs://neuron/data/bert_pred.tfrecords',\n",
    "      label_list = ['toxic', 'non-toxic'],\n",
    "      num_labels = 2,\n",
    "      max_seq_length = 512,\n",
    "      train_batch_size = 16 * 8,\n",
    "      eval_batch_size = 8,\n",
    "      predict_batch_size = 8 * 8,\n",
    "      eval_examples = 153134,\n",
    "      train_examples = 1304874,\n",
    "      pred_examples = 97320,\n",
    "      learning_rate = 5e-5,\n",
    "      num_train_epochs = 3.0,\n",
    "      warmup_proportion = 0.1,\n",
    "      save_checkpoints_steps = 1000,\n",
    "      keep_checkpoint_max = 2,\n",
    "      iteration_per_loop = 1000,\n",
    "      use_tpu = True,\n",
    "      tpu = 'grpc://10.0.101.2:8470',\n",
    "      tpu_zone = 'us-central1-c',\n",
    "      gcp_project = 'fluted-visitor-233103',\n",
    "      num_tpu_cores = 8\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Config for some predefined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(object):\n",
    "  \n",
    "  def __init__(self, vocab_size, hidden_size = 768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act='gelu', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=16, initializer_range=0.02):\n",
    "    self.vocab_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_hidden_layers = num_hidden_layers\n",
    "    self.num_attention_heads = num_attention_heads\n",
    "    self.hidden_act = hidden_act\n",
    "    self.intermediate_size = intermediate_size\n",
    "    self.hidden_dropout_prob = hidden_dropout_prob\n",
    "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "    self.max_position_embeddings = max_position_embeddings\n",
    "    self.type_vocab_size = type_vocab_size\n",
    "    self.initializer_range = initializer_range\n",
    "    \n",
    "  @classmethod  \n",
    "  def from_dict(cls, json_object):\n",
    "    config = BertConfig(vocab_size=None)\n",
    "    for (key, value) in six.iteritems(json_object):\n",
    "      config.__dict__[key] = value\n",
    "    return config\n",
    "  \n",
    "  @classmethod\n",
    "  def from_json_file(cls, json_file):\n",
    "    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
    "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
    "      text = reader.read()\n",
    "    return cls.from_dict(json.loads(text))\n",
    "  \n",
    "  def to_dict(self):\n",
    "    output = copy.deepcopy(self.__dict__)\n",
    "    return output\n",
    "  \n",
    "  def to_json_string(self):\n",
    "    return json.dumps(self.to_dict(), indent=2, sort_keys=True + \"\\n\")\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(object):\n",
    "  \n",
    "  def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=False, scope=None):\n",
    "    \n",
    "    config = copy.deepcopy(config)\n",
    "    if not is_training:\n",
    "      config.hidden_dropout_prob = 0.0\n",
    "      config.attention_probs_dropout_prob = 0.0\n",
    "      \n",
    "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "    \n",
    "    if input_mask is None:\n",
    "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "      \n",
    "      \n",
    "    if token_type_ids is None:\n",
    "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "      \n",
    "      \n",
    "    with tf.variable_scope(scope, default_name='bert'):\n",
    "      with tf.variable_scope('embeddings'):\n",
    "        (self.embedding_output, self.embedding_table) = embedding_lookup(input_ids=input_ids, vocab_size=config.vocab_size, embedding_size=config.hidden_size, initializer_range=config.initializer_range, word_embedding_name='word_embeddings', use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "        \n",
    "        \n",
    "        self.embedding_output = embedding_postprocessor(\n",
    "            input_tensor = self.embedding_output,\n",
    "            use_token_type=True,\n",
    "            token_type_ids=token_type_ids,\n",
    "            token_type_vocab_size=config.type_vocab_size,\n",
    "            token_type_embedding_name='token_type_embeddings',\n",
    "            use_position_embeddings=True,\n",
    "            position_embedding_name='position_embeddings',\n",
    "            initializer_range=config.initializer_range,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            dropout_prob=config.hidden_dropout_prob)\n",
    "        \n",
    "      with tf.variable_scope('encoder'):\n",
    "\n",
    "        attention_mask = create_attention_mask_from_input_mask(\n",
    "              input_ids, input_mask)\n",
    "\n",
    "        self.all_encoder_layers = transformer_model(\n",
    "          input_tensor=self.embedding_output,\n",
    "          attention_mask=attention_mask,\n",
    "          hidden_size=config.hidden_size,\n",
    "          num_hidden_layers=config.num_hidden_layers,\n",
    "          num_attention_heads=config.num_attention_heads,\n",
    "          intermediate_size=config.intermediate_size,\n",
    "          intermediate_act_fn=get_activation(config.hidden_act),\n",
    "          hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "          attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "          initializer_range=config.initializer_range,\n",
    "          do_return_all_layers=True)\n",
    "\n",
    "      self.sequence_output = self.all_encoder_layers[-1]\n",
    "\n",
    "      with tf.variable_scope('pooler'):\n",
    "\n",
    "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "        self.pooled_output = tf.layers.dense(\n",
    "            first_token_tensor,\n",
    "            config.hidden_size,\n",
    "            activation=tf.tanh,\n",
    "            kernel_initializer=create_initializer(config.initializer_range))\n",
    "\n",
    "  def get_pooled_output(self):\n",
    "    return self.pooled_output\n",
    "  \n",
    "  def get_sequence_output(self):\n",
    "    return self.sequence_output\n",
    "  \n",
    "  \n",
    "  def get_embedding_output(self):\n",
    "    return self.embedding_output\n",
    "  \n",
    "  def get_all_encoder_layers(self):\n",
    "    return self.all_encoder_layers\n",
    "  \n",
    "  def get_embedding_table(self):\n",
    "    return self.embedding_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "  cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "  return x * cdf\n",
    "\n",
    "def get_activation(activation_string):\n",
    "  \n",
    "  if not isinstance(activation_string, six.string_types):\n",
    "    return activation_string\n",
    "  \n",
    "  if not activation_string:\n",
    "    return None\n",
    "  \n",
    "  act = activation_string.lower()\n",
    "  if act == 'linear':\n",
    "    return None\n",
    "  elif act == 'relu':\n",
    "    return tf.nn.relu\n",
    "  elif act == 'gelu':\n",
    "    return gelu\n",
    "  elif act == 'tanh':\n",
    "    return tf.tanh\n",
    "  else:\n",
    "    raise ValueError('Unsupported activation: %s' % act)\n",
    "    \n",
    "    \n",
    "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
    "  \n",
    "  assignment_map = {}\n",
    "  initialized_variable_names = {}\n",
    "  \n",
    "  name_to_variable = collections.OrderedDict()\n",
    "  for var in tvars:\n",
    "    name = var.name\n",
    "    m = re.match('^(.*):\\\\d+$', name)\n",
    "    if m is not None:\n",
    "      name = m.group(1)\n",
    "    name_to_variable[name] = var\n",
    "    \n",
    "  init_vars = tf.train.list_variables(init_checkpoint)\n",
    "  \n",
    "  assignment_map = collections.OrderedDict()\n",
    "  for x in init_vars:\n",
    "    (name, var) = (x[0], x[1])\n",
    "    if name not in name_to_variable:\n",
    "      continue\n",
    "      \n",
    "    assignment_map[name] = name\n",
    "    initialized_variable_names[name] = 1\n",
    "    initialized_variable_names[name+\":0\"] = 1\n",
    "    \n",
    "  return (assignment_map, initialized_variable_names)\n",
    "\n",
    "\n",
    "def dropout(input_tensor, dropout_prob):\n",
    "  if dropout_prob is None or dropout_prob == 0.0:\n",
    "    return input_tensor\n",
    "  \n",
    "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
    "  return output\n",
    "\n",
    "\n",
    "def layer_norm(input_tensor, name=None):\n",
    "  return tf.contrib.layers.layer_norm(\n",
    "    inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
    "\n",
    "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
    "  \n",
    "  output_tensor = layer_norm(input_tensor, name)\n",
    "  output_tensor = dropout(output_tensor, dropout_prob)\n",
    "  return output_tensor\n",
    "\n",
    "\n",
    "def create_initializer(initializer_range=0.02):\n",
    "  return tf.truncated_normal_initializer(stddev=initializer_range)\n",
    "\n",
    "def embedding_lookup(input_ids, vocab_size, embedding_size=128, initializer_range=0.02, word_embedding_name='word_embeddings', use_one_hot_embeddings=False):\n",
    "  \n",
    "  if input_ids.shape.ndims == 2:\n",
    "    input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
    "    \n",
    "  embedding_table = tf.get_variable(\n",
    "      name=word_embedding_name,\n",
    "      shape=[vocab_size, embedding_size],\n",
    "      initializer=create_initializer(initializer_range))\n",
    "  \n",
    "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
    "  \n",
    "  if use_one_hot_embeddings:\n",
    "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
    "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
    "  else:\n",
    "    output = tf.gather(embedding_table, flat_input_ids)\n",
    "    \n",
    "  input_shape = get_shape_list(input_ids)\n",
    "  \n",
    "  output = tf.reshape(output,\n",
    "                     input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
    "  \n",
    "  return (output, embedding_table)\n",
    "\n",
    "\n",
    "def embedding_postprocessor(input_tensor,\n",
    "                           use_token_type=False,\n",
    "                           token_type_ids=None,\n",
    "                           token_type_vocab_size=16,\n",
    "                           token_type_embedding_name='token_type_embeddings',\n",
    "                           use_position_embeddings=True,\n",
    "                           position_embedding_name='position_embeddings',\n",
    "                           initializer_range=0.02,\n",
    "                           max_position_embeddings=512,\n",
    "                           dropout_prob=0.1):\n",
    "  \n",
    "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "  batch_size = input_shape[0]\n",
    "  seq_length = input_shape[1]\n",
    "  width = input_shape[2]\n",
    "  \n",
    "  output = input_tensor\n",
    "  \n",
    "  if use_token_type:\n",
    "    if token_type_ids is None:\n",
    "      raise ValueError('token_type_ids must be specified if use_token_type is True.')\n",
    "      \n",
    "    token_type_table = tf.get_variable(\n",
    "        name=token_type_embedding_name,\n",
    "        shape=[token_type_vocab_size, width],\n",
    "        initializer=create_initializer(initializer_range))\n",
    "    \n",
    "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
    "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
    "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
    "    token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])\n",
    "    \n",
    "    output += token_type_embeddings\n",
    "    \n",
    "    \n",
    "  if use_position_embeddings:\n",
    "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
    "    with tf.control_dependencies([assert_op]):\n",
    "      full_position_embeddings = tf.get_variable(\n",
    "          name=position_embedding_name,\n",
    "          shape=[max_position_embeddings, width],\n",
    "          initializer=create_initializer(initializer_range))\n",
    "      \n",
    "      position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n",
    "      num_dims = len(output.shape.as_list())\n",
    "      \n",
    "      position_broadcast_shape = []\n",
    "      for _ in range(num_dims - 2):\n",
    "        position_broadcast_shape.append(1)\n",
    "      position_broadcast_shape.extend([seq_length, width])\n",
    "      position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)\n",
    "      \n",
    "      output += position_embeddings\n",
    "      \n",
    "  output = layer_norm_and_dropout(output, dropout_prob)\n",
    "  return output\n",
    "\n",
    "\n",
    "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
    "  \n",
    "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "  batch_size = from_shape[0]\n",
    "  from_seq_length = from_shape[1]\n",
    "  \n",
    "  \n",
    "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
    "  to_seq_length = to_shape[1]\n",
    "  \n",
    "  to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
    "  \n",
    "  broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
    "  \n",
    "  mask = broadcast_ones * to_mask\n",
    "  \n",
    "  return mask\n",
    "\n",
    "def attention_layer(from_tensor,\n",
    "                    to_tensor,\n",
    "                    attention_mask=None,\n",
    "                    num_attention_heads=1,\n",
    "                    size_per_head=512,\n",
    "                    query_act=None,\n",
    "                    key_act=None,\n",
    "                    value_act=None,\n",
    "                    attention_probs_dropout_prob=0.0,\n",
    "                    initializer_range=0.02,\n",
    "                    do_return_2d_tensor=False,\n",
    "                    batch_size=None,\n",
    "                    from_seq_length=None,\n",
    "                    to_seq_length=None):\n",
    "  \n",
    "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width):\n",
    "    \n",
    "    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
    "    \n",
    "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "    return output_tensor\n",
    "  \n",
    "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
    "  \n",
    "  if len(from_shape) != len(to_shape):\n",
    "    raise ValueError('The rank of from_tensor must match the rank of to_tensor')\n",
    "    \n",
    "  if len(from_shape) == 3:\n",
    "    batch_size = from_shape[0]\n",
    "    from_seq_length = from_shape[1]\n",
    "    to_seq_length = to_shape[1]\n",
    "  elif len(from_shape) == 2:\n",
    "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
    "      raise ValueError(\"When passing in rank 2 tensors to attention_layer, the values \"\n",
    "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
    "          \"must all be specified.\")\n",
    "      \n",
    "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
    "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
    "  \n",
    "  query_layer = tf.layers.dense(\n",
    "      from_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=query_act,\n",
    "      name='query',\n",
    "      kernel_initializer=create_initializer(initializer_range))\n",
    "  \n",
    "  key_layer = tf.layers.dense(\n",
    "      to_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=key_act,\n",
    "      name='key',\n",
    "      kernel_initializer=create_initializer(initializer_range))\n",
    "  \n",
    "  value_layer = tf.layers.dense(\n",
    "      to_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=value_act,\n",
    "      name='value',\n",
    "      kernel_initializer=create_initializer(initializer_range))\n",
    "  \n",
    "  query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, from_seq_length, size_per_head)\n",
    "  \n",
    "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head)\n",
    "  \n",
    "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "  attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))\n",
    "  \n",
    "  if attention_mask is not None:\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
    "    \n",
    "    adder = (1.0 - tf.cast(attention_mask, tf.float32))\n",
    "    \n",
    "    attention_scores += adder\n",
    "    \n",
    "  attention_probs = tf.nn.softmax(attention_scores)\n",
    "  \n",
    "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
    "  \n",
    "  value_layer = tf.reshape(\n",
    "      value_layer,\n",
    "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
    "  \n",
    "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
    "  \n",
    "  context_layer = tf.matmul(attention_probs, value_layer)\n",
    "  \n",
    "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
    "  \n",
    "  if do_return_2d_tensor:\n",
    "    context_layer = tf.reshape(\n",
    "        context_layer,\n",
    "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
    "    \n",
    "  else:\n",
    "    context_layer = tf.reshape(\n",
    "        context_layer,\n",
    "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
    "    \n",
    "  return context_layer\n",
    "\n",
    "\n",
    "def transformer_model(input_tensor,\n",
    "                      attention_mask=None,\n",
    "                      hidden_size=768,\n",
    "                      num_hidden_layers=12,\n",
    "                      num_attention_heads=12,\n",
    "                      intermediate_size=3072,\n",
    "                      intermediate_act_fn=gelu,\n",
    "                      hidden_dropout_prob=0.1,\n",
    "                      attention_probs_dropout_prob=0.1,\n",
    "                      initializer_range=0.02,\n",
    "                      do_return_all_layers=False):\n",
    "  \n",
    "  \n",
    "  if hidden_size % num_attention_heads != 0:\n",
    "    raise ValueError(\"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "    \n",
    "  attention_head_size = int(hidden_size / num_attention_heads)\n",
    "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "  batch_size = input_shape[0]\n",
    "  seq_length = input_shape[1]\n",
    "  input_width = input_shape[2]\n",
    "  \n",
    "  if input_width != hidden_size:\n",
    "    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
    "                     (input_width, hidden_size))\n",
    "    \n",
    "  prev_output = reshape_to_matrix(input_tensor)\n",
    "  \n",
    "  all_layers_outputs = []\n",
    "  for layer_idx in range(num_hidden_layers):\n",
    "    with tf.variable_scope('layer_%d' % layer_idx):\n",
    "      layer_input = prev_output\n",
    "      \n",
    "      with tf.variable_scope('attention'):\n",
    "        attention_heads = []\n",
    "        with tf.variable_scope('self'):\n",
    "          attention_head = attention_layer(\n",
    "              from_tensor=layer_input,\n",
    "              to_tensor=layer_input,\n",
    "              attention_mask=attention_mask,\n",
    "              num_attention_heads=num_attention_heads,\n",
    "              size_per_head=attention_head_size,\n",
    "              attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "              initializer_range=initializer_range,\n",
    "              do_return_2d_tensor=True,\n",
    "              batch_size=batch_size,\n",
    "              from_seq_length=seq_length,\n",
    "              to_seq_length=seq_length)\n",
    "          attention_heads.append(attention_head)\n",
    "          \n",
    "        attention_output = None\n",
    "        if len(attention_heads) == 1:\n",
    "          attention_output = attention_heads[0]\n",
    "        else:\n",
    "          attention_output = tf.concat(attention_heads, axis=-1)\n",
    "          \n",
    "        with tf.variable_scope('output'):\n",
    "          attention_output = tf.layers.dense(\n",
    "              attention_output,\n",
    "              hidden_size,\n",
    "              kernel_initializer=create_initializer(initializer_range))\n",
    "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
    "          attention_output = layer_norm(attention_output + layer_input)\n",
    "          \n",
    "      with tf.variable_scope('intermediate'):\n",
    "        intermediate_output = tf.layers.dense(\n",
    "            attention_output,\n",
    "            intermediate_size,\n",
    "            activation=intermediate_act_fn,\n",
    "            kernel_initializer=create_initializer(initializer_range))\n",
    "        \n",
    "        \n",
    "      with tf.variable_scope('output'):\n",
    "        layer_output = tf.layers.dense(\n",
    "            intermediate_output,\n",
    "            hidden_size,\n",
    "            kernel_initializer=create_initializer(initializer_range))\n",
    "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
    "        layer_output = layer_norm(layer_output + attention_output)\n",
    "        prev_output = layer_output\n",
    "        all_layers_outputs.append(layer_output)\n",
    "        \n",
    "        \n",
    "  \n",
    "  if do_return_all_layers:\n",
    "    final_outputs = []\n",
    "    for layer_output in all_layers_outputs:\n",
    "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
    "      final_outputs.append(final_output)\n",
    "    return final_outputs\n",
    "  else:\n",
    "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
    "    return final_output\n",
    "  \n",
    "  \n",
    "  \n",
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "  \n",
    "  if name is None:\n",
    "    name = tensor.name\n",
    "    \n",
    "  if expected_rank is not None:\n",
    "    assert_rank(tensor, expected_rank, name)\n",
    "    \n",
    "  shape = tensor.shape.as_list()\n",
    "  \n",
    "  non_static_indexes = []\n",
    "  for (index, dim) in enumerate(shape):\n",
    "    if dim is None:\n",
    "      non_static_indexes.append(index)\n",
    "      \n",
    "  if not non_static_indexes:\n",
    "    return shape\n",
    "  \n",
    "  dyn_shape = tf.shape(tensor)\n",
    "  for index in non_static_indexes:\n",
    "    shape[index] = dyn_shape[index]\n",
    "  return shape\n",
    "\n",
    "def reshape_to_matrix(input_tensor):\n",
    "  ndims = input_tensor.shape.ndims\n",
    "  if ndims < 2:\n",
    "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
    "                     (input_tensor.shape))\n",
    "    \n",
    "  if ndims == 2:\n",
    "    return input_tensor\n",
    "  \n",
    "  width = input_tensor.shape[-1]\n",
    "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
    "  return output_tensor\n",
    "\n",
    "\n",
    "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
    "  \n",
    "  if len(orig_shape_list) == 2:\n",
    "    return output_tensor\n",
    "  \n",
    "  output_shape = get_shape_list(output_tensor)\n",
    "  \n",
    "  orig_dims = orig_shape_list[0:-1]\n",
    "  width = output_shape[-1]\n",
    "  \n",
    "  return tf.reshape(output_tensor, orig_dims + [width])\n",
    "\n",
    "\n",
    "def assert_rank(tensor, expected_rank, name=None):\n",
    "  \n",
    "  if name is None:\n",
    "    name = tensor.name\n",
    "    \n",
    "  expected_rank_dict = {}\n",
    "  if isinstance(expected_rank, six.integer_types):\n",
    "    expected_rank_dict[expected_rank] = True\n",
    "  else:\n",
    "    for x in expected_rank:\n",
    "      expected_rank_dict[x] = True\n",
    "      \n",
    "  actual_rank = tensor.shape.ndims\n",
    "  if actual_rank not in expected_rank_dict:\n",
    "    scope_name = tf.get_variable_scope().name\n",
    "    raise ValueError(\n",
    "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
    "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
    "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
    "  \n",
    "  global_step = tf.train.get_or_create_global_step()\n",
    "  \n",
    "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
    "  \n",
    "  learning_rate = tf.train.polynomial_decay(\n",
    "      learning_rate,\n",
    "      global_step,\n",
    "      num_train_steps,\n",
    "      end_learning_rate=0.0,\n",
    "      power=1.0,\n",
    "      cycle=False)\n",
    "  \n",
    "  if num_warmup_steps:\n",
    "    global_steps_int = tf.cast(global_step, tf.int32)\n",
    "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
    "    \n",
    "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "    \n",
    "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "    warmup_learning_rate = init_lr * warmup_percent_done\n",
    "    \n",
    "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "    learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "    \n",
    "    \n",
    "  optimizer = AdamWeightDecayOptimizer(\n",
    "      learning_rate = learning_rate,\n",
    "      weight_decay_rate = 0.01,\n",
    "      beta_1 = 0.9,\n",
    "      beta_2 = 0.999,\n",
    "      epsilon = 1e-6,\n",
    "      exclude_from_weight_decay = ['LayerNorm', 'layer_norm', 'bias'])\n",
    "  \n",
    "  if use_tpu:\n",
    "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "    \n",
    "  tvars = tf.trainable_variables()\n",
    "  grads = tf.gradients(loss, tvars)\n",
    "  \n",
    "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "  \n",
    "  train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "  \n",
    "  new_global_step = global_step + 1\n",
    "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
    "  \n",
    "  return train_op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT use Adam Optimizer with weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
    "  \n",
    "  def __init__(self, learning_rate, weight_decay_rate=0.0, beta_1=0.9, beta_2=0.999, epsilon=1e-6, exclude_from_weight_decay=None, name='AdamWeightDecayOptimizer'):\n",
    "    \n",
    "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
    "    \n",
    "    self.learning_rate = learning_rate\n",
    "    self.weight_decay_rate = weight_decay_rate\n",
    "    self.beta_1 = beta_1\n",
    "    self.beta_2 = beta_2\n",
    "    self.epsilon = epsilon\n",
    "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "    \n",
    "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "    \n",
    "    assignments = []\n",
    "    for (grad, param) in grads_and_vars:\n",
    "      if grad is None or param is None:\n",
    "        continue\n",
    "        \n",
    "      param_name = self._get_variable_name(param.name)\n",
    "      \n",
    "      m = tf.get_variable(\n",
    "          name=param_name + '/adam_m',\n",
    "          shape=param.shape.as_list(),\n",
    "          dtype=tf.float32,\n",
    "          trainable=False,\n",
    "          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      v = tf.get_variable(\n",
    "          name=param_name + '/adam_v',\n",
    "          shape=param.shape.as_list(),\n",
    "          dtype=tf.float32,\n",
    "          trainable=False,\n",
    "          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      next_m = (tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
    "      \n",
    "      next_v = (tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2, tf.square(grad)))\n",
    "      \n",
    "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
    "      \n",
    "      if self._do_use_weight_decay(param_name):\n",
    "        update += self.weight_decay_rate * param\n",
    "        \n",
    "      update_with_lr = self.learning_rate * update\n",
    "      \n",
    "      next_param = param - update_with_lr\n",
    "      \n",
    "      assignments.extend(\n",
    "          [param.assign(next_param),\n",
    "           m.assign(next_m),\n",
    "           v.assign(next_v)])\n",
    "      \n",
    "      return tf.group(*assignments, name=name)\n",
    "    \n",
    "  def _do_use_weight_decay(self, param_name):\n",
    "    \n",
    "    if not self.weight_decay_rate:\n",
    "      return False\n",
    "    if self.exclude_from_weight_decay:\n",
    "      for r in self.exclude_from_weight_decay:\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "        \n",
    "    return True\n",
    "  \n",
    "  def _get_variable_name(self, param_name):\n",
    "    m = re.match('^(.*):\\\\d+$', param_name)\n",
    "    if m is not None:\n",
    "      param_name = m.group(1)\n",
    "    return param_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model and calculate loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids, labels, num_labels, use_one_hot_embeddings):\n",
    "  \n",
    "  model = BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids = segment_ids,\n",
    "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "  \n",
    "  output_layer = model.get_pooled_output()\n",
    "  \n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "  output_weights = tf.get_variable(\n",
    "      'output_weights', [num_labels, hidden_size], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "  \n",
    "  output_bias = tf.get_variable(\n",
    "      'output_bias', [num_labels], initializer=tf.zeros_initializer())\n",
    "  \n",
    "  with tf.variable_scope('loss'):\n",
    "    if is_training:\n",
    "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "      \n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    \n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "    \n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    \n",
    "    return (loss, per_example_loss, logits, probabilities)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input function for training, evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
    "  \n",
    "  name_to_features = {\n",
    "      'input_ids': tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      'input_mask': tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      'segment_ids': tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      'label_ids': tf.FixedLenFeature([], tf.int64),\n",
    "      'is_real_example': tf.FixedLenFeature([], tf.int64)\n",
    "  }\n",
    "  \n",
    "  \n",
    "  def decode_record(record, name_to_features):\n",
    "    \n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "    \n",
    "    for name in list(example.keys()):\n",
    "      t = example[name]\n",
    "      if t.dtype == tf.int64:\n",
    "        t = tf.to_int32(t)\n",
    "      example[name] = t\n",
    "        \n",
    "    return example\n",
    "  \n",
    "  def input_fn(params):\n",
    "    \n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "      dataset = dataset.repeat()\n",
    "      dataset = dataset.shuffle(buffer_size=100)\n",
    "      \n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))\n",
    "    \n",
    "    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "  \n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function for TPUEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "  \n",
    "  tf.logging.info('*** Features ***')\n",
    "  for name in sorted(features.keys()):\n",
    "    tf.logging.info(' name = %s, shape = %s' % (name, features[name].shape))\n",
    "    \n",
    "  input_ids = features['input_ids']\n",
    "  input_mask = features['input_mask']\n",
    "  segment_ids = features['segment_ids']\n",
    "  label_ids = features['label_ids']\n",
    "  \n",
    "  is_real_example = None\n",
    "  if 'is_real_example' in features:\n",
    "    is_real_example = tf.cast(features['is_real_example'], dtype=tf.float32)\n",
    "  else:\n",
    "    is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "    \n",
    "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  \n",
    "  (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "      params['bert_config'], is_training, input_ids, input_mask, segment_ids, label_ids, params['num_labels'], params['use_tpu'])\n",
    "  \n",
    "  tvars = tf.trainable_variables()\n",
    "  initialized_variable_names = {}\n",
    "  scaffold_fn = None\n",
    "  \n",
    "  if params['init_checkpoint']:\n",
    "    (assignment_map, initialized_variable_names) = get_assignment_map_from_checkpoint(tvars, params['init_checkpoint'])\n",
    "    \n",
    "    if params['use_tpu']:\n",
    "      \n",
    "      def tpu_scaffold():\n",
    "        tf.train.init_from_checkpoint(params['init_checkpoint'], assignment_map)\n",
    "        return tf.train.Scaffold()\n",
    "      \n",
    "      scaffold_fn = tpu_scaffold\n",
    "      \n",
    "  tf.logging.info('*** Trainable Variables ***')\n",
    "  for var in tvars:\n",
    "    init_string = ''\n",
    "    if var.name in initialized_variable_names:\n",
    "      init_string = ', *INIT_FROM_CKPT*'\n",
    "    tf.logging.info(' name = %s, shape = %s%s', var.name, var.shape, init_string)\n",
    "\n",
    "    \n",
    "  output_spec = None\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    \n",
    "    train_op = create_optimizer(total_loss, params['learning_rate'], params['num_train_steps'], params['num_warmup_steps'], params['use_tpu'])\n",
    "    \n",
    "    output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=total_loss,\n",
    "        train_op=train_op,\n",
    "        scaffold_fn=scaffold_fn)\n",
    "    \n",
    "  elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "    \n",
    "    def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
    "      predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "      accuracy = tf.metrics.accuracy(\n",
    "          labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "      \n",
    "      loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "      return {\n",
    "          'eval_accuracy': accuracy,\n",
    "          'eval_loss': loss,\n",
    "      }\n",
    "    \n",
    "    eval_metrics = (metric_fn, [per_example_loss, label_ids, logits, is_real_example])\n",
    "    \n",
    "    output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=total_loss,\n",
    "        eval_metrics=eval_metrics,\n",
    "        scaffold_fn=scaffold_fn)\n",
    "    \n",
    "  else:\n",
    "    output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={'probabilities': probabilities},\n",
    "        scaffold_fn=scaffold_fn)\n",
    "    \n",
    "  return output_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_experimental_distribute': None, '_train_distribute': None, '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': 'grpc://10.0.101.2:8470', '_service': None, '_session_config': allow_soft_placement: true\n",
      "cluster_def {\n",
      "  job {\n",
      "    name: \"worker\"\n",
      "    tasks {\n",
      "      key: 0\n",
      "      value: \"10.0.101.2:8470\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", '_device_fn': None, '_protocol': None, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_save_checkpoints_steps': 1000, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8006401278>, '_save_summary_steps': 50, '_tf_random_seed': None, '_is_chief': True, '_evaluation_master': 'grpc://10.0.101.2:8470', '_num_worker_replicas': 1, '_save_checkpoints_secs': None, '_log_step_count_steps': None, '_model_dir': 'gs://neuron/bert', '_keep_checkpoint_every_n_hours': 10000, '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f803527a860>, '_task_type': 'worker', '_task_id': 0, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "INFO:tensorflow:*** Running training ***\n",
      "INFO:tensorflow: Num examples = 1304874\n",
      "INFO:tensorflow: Batch size = 128\n",
      "INFO:tensorflow: Num steps = 30582\n",
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.0.101.2:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 17292683551572988054)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5412835211617604124)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15692355304180925027)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1851466906463821093)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17919625023052869374)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1957569761356605834)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7878799601859577081)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11849021249349992968)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 11524721807636529348)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 1416194662470143425)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 82217998542886155)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:*** Features ***\n",
      "INFO:tensorflow: name = input_ids, shape = (16, 512)\n",
      "INFO:tensorflow: name = input_mask, shape = (16, 512)\n",
      "INFO:tensorflow: name = is_real_example, shape = (16,)\n",
      "INFO:tensorflow: name = label_ids, shape = (16,)\n",
      "INFO:tensorflow: name = segment_ids, shape = (16, 512)\n",
      "INFO:tensorflow:*** Trainable Variables ***\n",
      "INFO:tensorflow: name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow: name = output_weights:0, shape = (2, 768)\n",
      "INFO:tensorflow: name = output_bias:0, shape = (2,)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:TPU job name worker\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://neuron/bert/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into gs://neuron/bert/model.ckpt.\n",
      "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
      "INFO:tensorflow:Installing graceful shutdown hook.\n",
      "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
      "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
      "\n",
      "INFO:tensorflow:Init TPU system\n",
      "INFO:tensorflow:Initialized TPU in 3 seconds\n",
      "INFO:tensorflow:Starting infeed thread controller.\n",
      "INFO:tensorflow:Starting outfeed thread controller.\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into gs://neuron/bert/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.15426843, step = 6000\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
      "INFO:tensorflow:Saving checkpoints for 7000 into gs://neuron/bert/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.27231914, step = 7000 (697.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.43408\n",
      "INFO:tensorflow:examples/sec: 183.562\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
      "INFO:tensorflow:Saving checkpoints for 8000 into gs://neuron/bert/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.082898766, step = 8000 (636.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.57107\n",
      "INFO:tensorflow:examples/sec: 201.097\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
      "INFO:tensorflow:Saving checkpoints for 9000 into gs://neuron/bert/model.ckpt.\n",
      "WARNING:tensorflow:From /home/nikhil_neuron_gmail_com/.local/lib/python3.5/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:loss = 0.5583535, step = 9000 (679.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.47119\n",
      "INFO:tensorflow:examples/sec: 188.312\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into gs://neuron/bert/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13413718, step = 10000 (532.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.87939\n",
      "INFO:tensorflow:examples/sec: 240.563\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
      "INFO:tensorflow:Saving checkpoints for 11000 into gs://neuron/bert/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.23093188, step = 11000 (554.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.80433\n",
      "INFO:tensorflow:examples/sec: 230.955\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
      "INFO:tensorflow:Saving checkpoints for 12000 into gs://neuron/bert/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.36425668, step = 12000 (509.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.96361\n",
      "INFO:tensorflow:examples/sec: 251.342\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
      "INFO:tensorflow:Saving checkpoints for 13000 into gs://neuron/bert/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.121045634, step = 13000 (509.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.96398\n",
      "INFO:tensorflow:examples/sec: 251.389\n",
      "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
      "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "  hparams = get_hparams()\n",
    "  \n",
    "  \n",
    "  bert_config = BertConfig.from_json_file(hparams.bert_config_file)\n",
    "  num_train_steps = None\n",
    "  num_warmup_steps = None\n",
    "  \n",
    "  num_train_steps = int(hparams.train_examples / hparams.train_batch_size * hparams.num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * hparams.warmup_proportion)\n",
    "  \n",
    "  params = dict(\n",
    "      hparams.values(),\n",
    "      num_train_steps = num_train_steps,\n",
    "      num_warmup_steps = num_warmup_steps,\n",
    "      bert_config = bert_config\n",
    "  )\n",
    "  \n",
    "  \n",
    "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "      params['tpu'], zone=params['tpu_zone'], project=params['gcp_project'])\n",
    "  \n",
    "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  \n",
    "  run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      model_dir=params['model_dir'],\n",
    "      save_checkpoints_steps=params['save_checkpoints_steps'],\n",
    "      save_summary_steps=50,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          iterations_per_loop=params['iteration_per_loop'],\n",
    "          num_shards=params['num_tpu_cores'],\n",
    "          per_host_input_for_training=is_per_host\n",
    "      )\n",
    "  )\n",
    "  \n",
    "  estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=params['use_tpu'],\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size=params['train_batch_size'],\n",
    "      params=params\n",
    "  )\n",
    "  \n",
    "  tf.logging.info('*** Running training ***')\n",
    "  tf.logging.info(' Num examples = %d', params['train_examples'])\n",
    "  tf.logging.info(' Batch size = %d', params['train_batch_size'])\n",
    "  tf.logging.info(' Num steps = %d', num_train_steps)\n",
    "  \n",
    "  input_fn = input_fn_builder(\n",
    "      input_file=hparams.train_file,\n",
    "      seq_length=hparams.max_seq_length,\n",
    "      is_training=True,\n",
    "      drop_remainder=True\n",
    "  )\n",
    "  \n",
    "  estimator.train(input_fn=input_fn, max_steps=num_train_steps)\n",
    "  "
   ]
  },
  
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
