{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text into tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKZZyaauijT5"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import unicodedata\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the 'train.csv' file from this link: https://drive.google.com/open?id=1iLyI3BgjO8H5tf96WI__C1MmOrujzAtE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKbyT_6TFuuy"
   },
   "outputs": [],
   "source": [
    "toxic = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0WSG5zu4Ep0"
   },
   "source": [
    "#### convert csv file to tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqTAFLfSTlUs"
   },
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "\n",
    "for i in range(97320):\n",
    "  example = collections.OrderedDict()\n",
    "  text_id = toxic['id'][i]\n",
    "  text = toxic['comment_text'][i]\n",
    "  label_id = toxic_label['id'][i]\n",
    "  label = toxic_label['prediction'][i]\n",
    "  \n",
    "  if re.search(str(text_id), str(label_id)):\n",
    "  \n",
    "    st = ''\n",
    "    split = text.split('\\n')\n",
    "    for j in range(len(split)):\n",
    "      if split[j] != '':\n",
    "        st += split[j]\n",
    "        \n",
    "    example['comment_text'] = st\n",
    "    example['target'] = float(label)\n",
    "    train_examples.append(example)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QW2Jsx0s7S5n"
   },
   "outputs": [],
   "source": [
    "train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgB74Bkpk16u"
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FfLgg5Sok9H4"
   },
   "outputs": [],
   "source": [
    "train.to_csv('train.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "791yPBUV1kW9"
   },
   "source": [
    "### **Utility functions for preprocessing the data before convert to tfrecords**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBvP1_Wyh6ag"
   },
   "outputs": [],
   "source": [
    "def convert_to_unicode(text):\n",
    "  \n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode('utf-8', 'ignore')\n",
    "    else:\n",
    "      raise ValueError('Unsupported string type: ', type(text))\n",
    "  elif six.PY2: \n",
    "    if isinstance(text, str): \n",
    "      return text\n",
    "    elif isinstance(text, unicode):\n",
    "      return text.encode(\"utf-8\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError('Not running on Python2 or Python3?')\n",
    "      \n",
    "  \n",
    "  \n",
    "def printable_text(text):\n",
    "  \n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode('utf-8', 'ignore')\n",
    "    else:\n",
    "      raise ValueError('Unsupported string type: ', type(text))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, unicode):\n",
    "      return text.encode(\"utf-8\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "      \n",
    "def load_vocab(vocab_file):\n",
    "  vocab = collections.OrderedDict()\n",
    "  index = 0\n",
    "  with tf.gfile.GFile(vocab_file, 'r') as reader:\n",
    "    while True:\n",
    "      token = convert_to_unicode(reader.readline())\n",
    "      if not token:\n",
    "        break\n",
    "        \n",
    "      token = token.strip()\n",
    "      vocab[token] = index\n",
    "      index += 1\n",
    "      \n",
    "  return vocab\n",
    "\n",
    "\n",
    "def convert_by_vocab(vocab, items):\n",
    "  #print(items)\n",
    "  \n",
    "  output = []\n",
    "  for item in items:\n",
    "    output.append(vocab[item])\n",
    "  return output\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "  return convert_by_vocab(vocab, tokens)\n",
    "\n",
    "def convert_ids_to_tokens(inv_vocab, ids):\n",
    "  return convert_by_vocab(inv_vocab, ids)\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "  text = text.strip()\n",
    "  if not text:\n",
    "    return []\n",
    "  tokens = text.split()\n",
    "  return tokens\n",
    "\n",
    "\n",
    "class FullTokenizer(object):\n",
    "  \n",
    "  def __init__(self, vocab_file, do_lower_case=True):\n",
    "    self.vocab = load_vocab(vocab_file)\n",
    "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "    \n",
    "    \n",
    "  def tokenize(self, text):\n",
    "    split_tokens = []\n",
    "    for token in self.basic_tokenizer.tokenize(text):\n",
    "      for sub_tokens in self.wordpiece_tokenizer.tokenize(token):\n",
    "        split_tokens.append(sub_tokens)\n",
    "        \n",
    "        \n",
    "    return split_tokens\n",
    "  \n",
    "  def convert_tokens_to_ids(self, tokens):\n",
    "    return convert_by_vocab(self.vocab, tokens)\n",
    "  \n",
    "  def convert_ids_to_tokens(self, ids):\n",
    "    return convert_by_vocab(self.inv_vocab, ids)\n",
    "  \n",
    "  \n",
    "class BasicTokenizer(object):\n",
    "  \n",
    "  def __init__(self, do_lower_case=True):\n",
    "    self.do_lower_case = do_lower_case\n",
    "    \n",
    "  def tokenize(self, text):\n",
    "    text = convert_to_unicode(text)\n",
    "    text = self._clean_text(text)\n",
    "    \n",
    "    text = self._tokenize_chinese_chars(text)\n",
    "    \n",
    "    orig_tokens = whitespace_tokenize(text)\n",
    "    split_tokens = []\n",
    "    for token in orig_tokens:\n",
    "      if self.do_lower_case:\n",
    "        token = token.lower()\n",
    "        token = self._run_strip_accents(token)\n",
    "      split_tokens.extend(self._run_split_on_punc(token))\n",
    "      \n",
    "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "    return output_tokens\n",
    "  \n",
    "  def _run_strip_accents(self, text):\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cat = unicodedata.category(char)\n",
    "      if cat == 'Mn':\n",
    "        continue\n",
    "      output.append(char)\n",
    "    return \"\".join(output)\n",
    "  \n",
    "  def _run_split_on_punc(self, text):\n",
    "    chars = list(text)\n",
    "    i = 0\n",
    "    start_new_word = True\n",
    "    output = []\n",
    "    while i < len(chars):\n",
    "      char = chars[i]\n",
    "      if _is_punctuation(char):\n",
    "        output.append([char])\n",
    "        start_new_word = True\n",
    "      else:\n",
    "        if start_new_word:\n",
    "          output.append([])\n",
    "        start_new_word = False\n",
    "        output[-1].append(char)\n",
    "        \n",
    "      i += 1\n",
    "      \n",
    "    return [\"\".join(x) for x in output]\n",
    "  \n",
    "  def _tokenize_chinese_chars(self, text):\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cp = ord(char)\n",
    "      if self._is_chinese_char(cp):\n",
    "        output.append(\" \")\n",
    "        output.append(char)\n",
    "        output.append(\" \")\n",
    "      else:\n",
    "        output.append(char)\n",
    "        \n",
    "    return \"\".join(output)\n",
    "  \n",
    "  def _is_chinese_char(self, cp):\n",
    "    \n",
    "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "      return True\n",
    "\n",
    "    return False\n",
    "  \n",
    "  def _clean_text(self, text):\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cp = ord(char)\n",
    "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "        continue\n",
    "      if _is_whitespace(char):\n",
    "        output.append(\" \")\n",
    "        \n",
    "      else:\n",
    "        output.append(char)\n",
    "        \n",
    "    return \"\".join(output)\n",
    "  \n",
    "  \n",
    "class WordpieceTokenizer(object):\n",
    "  \n",
    "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "    \n",
    "    self.vocab = vocab\n",
    "    self.unk_token = unk_token\n",
    "    self.max_input_chars_per_word = max_input_chars_per_word\n",
    "    \n",
    "    \n",
    "  def tokenize(self, text):\n",
    "    \n",
    "    text = convert_to_unicode(text)\n",
    "    #print(text)\n",
    "    \n",
    "    output_tokens = []\n",
    "    for token in whitespace_tokenize(text):\n",
    "      chars = list(token)\n",
    "      if len(chars) > self.max_input_chars_per_word:\n",
    "        output_tokens.append(self.unk_token)\n",
    "        continue\n",
    "        \n",
    "      #print(output_tokens)\n",
    "        \n",
    "      is_bad = False\n",
    "      start = 0\n",
    "      sub_tokens = []\n",
    "      while start < len(chars):\n",
    "        end = len(chars)\n",
    "        cur_substr = None\n",
    "        while start < end:\n",
    "          substr = \"\".join(chars[start:end])\n",
    "          if start > 0:\n",
    "            substr = \"##\" + substr\n",
    "          if substr in self.vocab:\n",
    "            cur_substr = substr\n",
    "            break\n",
    "          end -= 1\n",
    "        if cur_substr is None:\n",
    "          is_bad = True\n",
    "          break\n",
    "          \n",
    "        sub_tokens.append(cur_substr)\n",
    "        start = end\n",
    "        \n",
    "      if is_bad:\n",
    "        output_tokens.append(self.unk_token)\n",
    "        \n",
    "      else:\n",
    "        output_tokens.extend(sub_tokens)\n",
    "        \n",
    "    return output_tokens\n",
    "  \n",
    "  \n",
    "  \n",
    "def _is_whitespace(char):\n",
    "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat == \"Zs\":\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def _is_control(char):\n",
    "  \n",
    "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return False\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat in (\"Cc\", \"Cf\"):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def _is_punctuation(char):\n",
    "  \n",
    "  cp = ord(char)\n",
    "  \n",
    "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat.startswith(\"P\"):\n",
    "    return True\n",
    "  return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSajplkVr0XO"
   },
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "  \n",
    "  def __init__(self, input_ids, input_mask, segment_ids, label_id, is_real_example=True):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    self.is_real_example = is_real_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_i6LDQI4VeE"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "  \n",
    "  def __init__(self, guid, text_a, text_b = None, label = None):\n",
    "    \n",
    "    self.guid = guid\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLIfdQhmokBL"
   },
   "outputs": [],
   "source": [
    "def read_tsv(input_file, quotechar=None):\n",
    "  with tf.gfile.Open(input_file, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "    lines = []\n",
    "    for (idx, line) in enumerate(reader):\n",
    "      data = []\n",
    "      com = ''\n",
    "      target = 0.0\n",
    "      if len(line) != 1:\n",
    "        for (i, ln) in enumerate(line):\n",
    "          ln_split = ln.split(',')\n",
    "          if ',' in ln and i==0:\n",
    "            for k in ln_split[1:]:\n",
    "              com += k\n",
    "            com += ' '\n",
    "          elif ',' in ln and i==len(line)-1:\n",
    "            for k in ln_split[:len(ln_split)-1]:\n",
    "              com += k\n",
    "            target = float(ln_split[len(ln_split)-1])\n",
    "          elif ',' in ln:\n",
    "            for k in ln_split:\n",
    "              com += k\n",
    "            com += ' '\n",
    "          elif ',' not in ln:\n",
    "            for k in ln_split:\n",
    "              com += k\n",
    "            com += ' '\n",
    "        data.append(com)\n",
    "        data.append(target)\n",
    "            \n",
    "      else:\n",
    "        if idx != 0:\n",
    "          line = line[0].split(',')\n",
    "          #data = []\n",
    "          if len(line) != 3:\n",
    "            target = float(line[len(line)-1])\n",
    "            comment = line[1:len(line)-1]\n",
    "            text = ''\n",
    "            for ln in comment:\n",
    "              text += ln\n",
    "            data.append(text)\n",
    "            data.append(target)\n",
    "\n",
    "          elif len(line) == 3:\n",
    "            data.append(line[1])\n",
    "            data.append(float(line[2]))\n",
    "\n",
    "          else:\n",
    "            raise ValueError('Value cannot be Null ', line)\n",
    "          \n",
    "      #print(idx, data)\n",
    "      lines.append(data)\n",
    "    lines = lines[1:]\n",
    "    #print(lines)\n",
    "      \n",
    "    return lines\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLC8N2xnp12c"
   },
   "outputs": [],
   "source": [
    "def create_examples(lines, set_type):\n",
    "  examples = []\n",
    "  for (i, line) in enumerate(lines):\n",
    "    \n",
    "    guid = '%s-%s' % (set_type, i)\n",
    "    text_a = convert_to_unicode(line[0])\n",
    "    if line[1] >= 0.5:\n",
    "      label = 'toxic'\n",
    "    else:\n",
    "      label = 'non-toxic'\n",
    "      \n",
    "    label = convert_to_unicode(label)\n",
    "    examples.append(InputExample(guid=guid, text_a = text_a, text_b = None, label = label))\n",
    "    \n",
    "  return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUuQs84T2QEr"
   },
   "source": [
    "#### change the length of tokens which is equal to max_sequence_length=512 and pad the tokens if length is not equals to max_sequence_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ifd4t9DrWKC"
   },
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "  while True:\n",
    "    total_length = len(token_a) + len(token_b)\n",
    "    if total_length <= max_length:\n",
    "      break\n",
    "    if len(tokens_a) > len(tokens_b):\n",
    "      tokens_a.pop()\n",
    "    else:\n",
    "      tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nomn7Vwx2_v_"
   },
   "source": [
    "#### convert the strings into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BiNmBv2t8KU"
   },
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer):\n",
    "  \n",
    "  label_map = {}\n",
    "  for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "    \n",
    "  #print(example.text_a)\n",
    "    \n",
    "  tokens_a = tokenizer.tokenize(example.text_a)\n",
    "  tokens_b = None\n",
    "  \n",
    "  if example.text_b:\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "    \n",
    "  if tokens_b:\n",
    "    truncate_seq_pair(tokens_a, tokens_b, max_length - 3)\n",
    "  else:\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "      \n",
    "  #print(tokens_a)\n",
    "  tokens = []\n",
    "  segment_ids = []\n",
    "  tokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "  tokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "  \n",
    "  if tokens_b:\n",
    "    for token in tokens_b:\n",
    "      tokens.append(token)\n",
    "      segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "    \n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  input_mask = [1] * len(input_ids)\n",
    "  \n",
    "  while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "    \n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "  \n",
    "  label_id = label_map[example.label]\n",
    "  if ex_index < 2 :\n",
    "    tf.logging.info(\"*** Example ***\")\n",
    "    tf.logging.info('guid: %s' % (example.guid))\n",
    "    tf.logging.info('tokens: %s' % \" \".join([printable_text(x) for x in tokens]))\n",
    "    tf.logging.info('input_ids: %s' % \" \".join([str(x) for x in input_ids]))\n",
    "    tf.logging.info('input_mask: %s' % \" \".join([str(x) for x in input_mask]))\n",
    "    tf.logging.info('segment_ids: %s' % \" \".join([str(x) for x in segment_ids]))\n",
    "    tf.logging.info('label: %s (id = %d)' % (example.label, label_id))\n",
    "  \n",
    "  feature = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label_id, is_real_example=True)\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdbX1lht3Vah"
   },
   "source": [
    "#### convert the data into tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGEC7Dc_1hhj"
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "  if not isinstance(value, list):\n",
    "    value = [value]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "  \n",
    "  writer = tf.python_io.TFRecordWriter(output_file)\n",
    "  \n",
    "  for (ex_index, example) in enumerate(examples):\n",
    "   \n",
    "    #if ex_index > 500000:\n",
    "      \n",
    "    if ex_index %10000 == 0:\n",
    "      tf.logging.info('Writing example %d of %d' % (ex_index, len(examples)))\n",
    "\n",
    "    feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features['input_ids'] = _int64_feature(feature.input_ids)\n",
    "    features['input_mask'] = _int64_feature(feature.input_mask)\n",
    "    features['segment_ids'] = _int64_feature(feature.segment_ids)\n",
    "    features['label_ids'] = _int64_feature(feature.label_id)\n",
    "    features['is_real_example'] = _int64_feature([int(feature.is_real_example)])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 178825,
     "status": "ok",
     "timestamp": 1554301217088,
     "user": {
      "displayName": "Rahul Mehra",
      "photoUrl": "",
      "userId": "00613151215397872246"
     },
     "user_tz": -330
    },
    "id": "A1JcWYvR4JL6",
    "outputId": "25b38541-608d-45b0-ccf0-6f66b7fa31c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 97320\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: pred-0\n",
      "INFO:tensorflow:tokens: [CLS] jeff sessions is another one of trump ' s or ##well ##ian choices . he believes and has believed his entire career the exact opposite of what the position requires . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 5076 6521 2003 2178 2028 1997 8398 1005 1055 2030 4381 2937 9804 1012 2002 7164 1998 2038 3373 2010 2972 2476 1996 6635 4500 1997 2054 1996 2597 5942 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: toxic (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: pred-1\n",
      "INFO:tensorflow:tokens: [CLS] \" i actually inspected the infrastructure on grand chief stewart philip ' s home pen ##tic ##ton first nation in both 2010 and 2013 . exactly zero projects that had been identified in previous inspection reports had been funded by the federal government and the entire band was housed in at ##co trailers . clearly the harper conservatives had already reduced the cash his band was sent to zero . \" [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1000 1045 2941 20456 1996 6502 2006 2882 2708 5954 5170 1005 1055 2188 7279 4588 2669 2034 3842 1999 2119 2230 1998 2286 1012 3599 5717 3934 2008 2018 2042 4453 1999 3025 10569 4311 2018 2042 6787 2011 1996 2976 2231 1998 1996 2972 2316 2001 7431 1999 2012 3597 21389 1012 4415 1996 8500 11992 2018 2525 4359 1996 5356 2010 2316 2001 2741 2000 5717 1012 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: toxic (id = 0)\n",
      "INFO:tensorflow:Writing example 10000 of 97320\n",
      "INFO:tensorflow:Writing example 20000 of 97320\n",
      "INFO:tensorflow:Writing example 30000 of 97320\n",
      "INFO:tensorflow:Writing example 40000 of 97320\n",
      "INFO:tensorflow:Writing example 50000 of 97320\n",
      "INFO:tensorflow:Writing example 60000 of 97320\n",
      "INFO:tensorflow:Writing example 70000 of 97320\n",
      "INFO:tensorflow:Writing example 80000 of 97320\n",
      "INFO:tensorflow:Writing example 90000 of 97320\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "  train_file = 'bert_train.tfrecords'\n",
    "  vocab_file = 'vocab.txt'\n",
    "  tsv_file = 'train.tsv'\n",
    "  max_seq_length = 512\n",
    "  do_lower_case = True\n",
    "  label_list = ['toxic', 'non-toxic']\n",
    "  \n",
    "  tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "  \n",
    "  lines = read_tsv(tsv_file)\n",
    "  train_examples = create_examples(lines, 'train')\n",
    "  \n",
    "  file_based_convert_examples_to_features(train_examples, label_list, max_seq_length, tokenizer, train_file)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZaWiA113bpa"
   },
   "source": [
    "### validate the tfrecords file so that no corrupted record exists in tfrecords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvyyhLFsBdZx"
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "from crcmod.predefined import mkPredefinedCrcFun\n",
    "\n",
    "_crc_fn = mkPredefinedCrcFun('crc-32c')\n",
    "\n",
    "\n",
    "def calc_masked_crc(data):\n",
    "    crc = _crc_fn(data)\n",
    "    return (((crc >> 15) | (crc << 17)) + 0xa282ead8) & 0xFFFFFFFF\n",
    "\n",
    "\n",
    "def validate_dataset(filename):\n",
    "    total_records = 0\n",
    "    total_bad_len_crc = 0\n",
    "    total_bad_data_crc = 0\n",
    "    #for f_name in filenames:\n",
    "    i = 0\n",
    "    print('validating ', filename)\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "\n",
    "        len_bytes = f.read(8)\n",
    "        while len(len_bytes) > 0:\n",
    "            # tfrecord format is a wrapper around protobuf data\n",
    "            length, = struct.unpack('<Q', len_bytes) # u64: length of the protobuf data (excluding the header)\n",
    "            len_crc, = struct.unpack('<I', f.read(4)) # u32: masked crc32c of the length bytes\n",
    "            data = f.read(length) # protobuf data\n",
    "            data_crc, = struct.unpack('<I', f.read(4)) # u32: masked crc32c of the protobuf data\n",
    "\n",
    "            if len_crc != calc_masked_crc(len_bytes):\n",
    "                print('bad crc on len at record', i)\n",
    "                total_bad_len_crc += 1\n",
    "\n",
    "            if data_crc != calc_masked_crc(data):\n",
    "                print('bad crc on data at record', i)\n",
    "                total_bad_data_crc += 1\n",
    "\n",
    "            i += 1\n",
    "            len_bytes = f.read(8)\n",
    "\n",
    "    print('checked', i, 'records')\n",
    "    total_records += i\n",
    "    print('checked', total_records, 'total records')\n",
    "    print('total with bad length crc: ', total_bad_len_crc)\n",
    "    print('total with bad data crc: ', total_bad_data_crc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opkZVs8FAW6U"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  validate_dataset('bert_train.tfrecords')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_tfrecords.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
